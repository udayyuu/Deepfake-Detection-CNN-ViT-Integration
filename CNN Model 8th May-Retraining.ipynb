{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ba07f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60580 files belonging to 2 classes.\n",
      "Using 59975 files for training.\n",
      "Using 605 files for validation.\n",
      "Found 13231 files belonging to 2 classes.\n",
      "Found 15620 files belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - acc: 0.7420 - loss: 0.5457 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 17:28:47.576654: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 34330240 bytes after encountering the first element of size 34330240 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23288s\u001b[0m 12s/step - acc: 0.7420 - loss: 0.5456 - val_acc: 0.6644 - val_loss: 0.9415\n",
      "Epoch 2/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23076s\u001b[0m 12s/step - acc: 0.9369 - loss: 0.1683 - val_acc: 0.6964 - val_loss: 1.0120\n",
      "Epoch 3/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - acc: 0.9670 - loss: 0.0947 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 06:12:35.814654: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 34330240 bytes after encountering the first element of size 34330240 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22657s\u001b[0m 12s/step - acc: 0.9670 - loss: 0.0947 - val_acc: 0.6872 - val_loss: 1.4536\n",
      "Epoch 4/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22735s\u001b[0m 12s/step - acc: 0.9773 - loss: 0.0658 - val_acc: 0.6482 - val_loss: 1.9159\n",
      "Epoch 5/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22755s\u001b[0m 12s/step - acc: 0.9820 - loss: 0.0547 - val_acc: 0.6700 - val_loss: 1.7911\n",
      "Epoch 6/25\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23100s\u001b[0m 12s/step - acc: 0.9862 - loss: 0.0413 - val_acc: 0.6848 - val_loss: 1.6929\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 17 21:38:24 2024\n",
    "\n",
    "@author: roysu\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "from keras import models ,layers\n",
    "from warnings import filterwarnings\n",
    "from keras.models import load_model\n",
    "filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "image_size = (299, 299)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds , unused_val = keras.utils.image_dataset_from_directory(\n",
    "    '../Desktop/Data/train_1',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    seed=1337,\n",
    "    subset='both',\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.01)\n",
    "\n",
    "val_ds = keras.utils.image_dataset_from_directory(\n",
    "    '../Desktop/Data/test_1',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.image_dataset_from_directory(\n",
    "    '../Desktop/Data/test_2',\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "train_ds = train_ds.prefetch(tf_data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf_data.AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(tf_data.AUTOTUNE)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# input_shape = (299, 299, 3)  # Input shape of the Xception model\n",
    "# num_classes = 2  # Number of output classes for the Xception model\n",
    "\n",
    "# #def Xception(input_shape, num_classes):\n",
    "# inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "# # Entry flow\n",
    "# x = layers.Conv2D(32, (3, 3), strides=(2, 2), use_bias=False)(inputs)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = layers.Dropout(.2)(x)\n",
    "# x = layers.Conv2D(64, (3, 3), use_bias=False)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = layers.Dropout(.2)(x)\n",
    "\n",
    "# previous_block_activation = x\n",
    "\n",
    "# for size in [128, 256, 728]:\n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     x = layers.SeparableConv2D(size, (3, 3), padding='same', use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     #x = layers.Dropout(.2)(x)\n",
    "\n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     x = layers.SeparableConv2D(size, (3, 3), padding='same', use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "\n",
    "#     x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "#     # Project residual\n",
    "#     residual = layers.Conv2D(size, (1, 1), strides=(2, 2), padding='same')(previous_block_activation)\n",
    "#     x = layers.add([x, residual])\n",
    "#     previous_block_activation = x\n",
    "\n",
    "# # Middle flow\n",
    "# for _ in range(8):\n",
    "#     residual = x\n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    \n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     #x = layers.Dropout(.3)(x)\n",
    "#     x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "\n",
    "#     x = layers.Activation('relu')(x)\n",
    "#     #x = layers.Dropout(.3)(x)\n",
    "#     x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "\n",
    "#     x = layers.add([x, residual])\n",
    "\n",
    "# # Exit flow\n",
    "# residual = layers.Conv2D(1024, (1, 1), strides=(2, 2), padding='same')(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = layers.Dropout(.2)(x)\n",
    "# x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = layers.Dropout(.2)(x)\n",
    "# x = layers.SeparableConv2D(1024, (3, 3), padding='same', use_bias=False)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "\n",
    "# x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "# x = layers.add([x, residual])\n",
    "\n",
    "# x = layers.SeparableConv2D(1536, (3, 3), padding='same', use_bias=False)(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "# #x = layers.Dropout(.2)(x)\n",
    "# # x = layers.SeparableConv2D(2048, (3, 3), padding='same', use_bias=False)(x)\n",
    "# # x = layers.BatchNormalization()(x)\n",
    "# # x = layers.Activation('relu')(x)\n",
    "# # x = layers.Dropout(.2)(x)\n",
    "# # Output layer\n",
    "# x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# x = layers.Dense(128, activation='relu') (x)\n",
    "\n",
    "# x =layers.Dropout(0.2)(x)\n",
    "\n",
    "# x = layers.Dense(32, activation= 'relu')(x)\n",
    "\n",
    "# x =layers.Dropout(0.2)(x)\n",
    "\n",
    "# outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# # Create the model\n",
    "# xception_model = models.Model(inputs, outputs)\n",
    "\n",
    "# xception_model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(3e-4),\n",
    "#     loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#     metrics=[keras.metrics.BinaryAccuracy(name=\"acc\")],\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# # Display model summary\n",
    "# xception_model.summary()\n",
    "\n",
    "Retrain_Xception=load_model('./7th_May_xception_model_on_FF++Tr1_at_5.keras')\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('./8th_May_xception_model_on_FF++Tr1_at_{epoch}.keras',\n",
    "                                    monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    save_freq=\"epoch\"),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                  patience=5,\n",
    "                                  mode=\"min\"),\n",
    "    keras.callbacks.CSVLogger('8th_May_xception_model_on_FF++Tr1_logs.csv',append=False)\n",
    "]\n",
    "\n",
    "history=Retrain_Xception.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_ds,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23103893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2590/2590\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 84ms/step - acc: 0.7248 - loss: 2.2388\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.273427963256836, 0.7281853556632996]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Retrain_Xception.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c286e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
